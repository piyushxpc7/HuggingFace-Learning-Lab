{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMRoNKlNnO66",
        "outputId": "f4f2eef3-979f-4615-cb3a-65dcaa17db35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch device available: cuda\n",
            "\n",
            "================ 1) SENTIMENT ANALYSIS ================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline output: [{'label': 'POSITIVE', 'score': 0.724334180355072}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model+tokenizer label: POSITIVE\n",
            "\n",
            "================ 2) QUESTION ANSWERING ================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline output: {'score': 0.9752811193466187, 'start': 73, 'end': 91, 'answer': '25 billion dollars'}\n",
            "Model+tokenizer answer: 25 billion dollars\n",
            "\n",
            "================ 3) SUMMARIZATION ================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline summary: [{'summary_text': 'Microsoft released its quarterly results today. The company reported strong growth in its cloud business Azure, while personal computing revenue was flat.'}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1633: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (41). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length. Note that `max_length` is set to 41, its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model+tokenizer summary: Microsoft released its quarterly results today. The company reported strong growth in its cloud business Azure, while personal computing revenue was flat. Overall, total revenue increased compared to the same quarter last year.\n",
            "\n",
            "================ 4) TRANSLATION ================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline translation: [{'translation_text': 'Die Zentralbank beschloss, die Zinssätze um 0,5% zu erhöhen.'}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model+tokenizer translation: Die Zentralbank beschloss, die Zinssätze um 0,5% zu erhöhen.\n",
            "\n",
            "================ 5) NER (ENTITY RECOGNITION) ================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline NER: [{'entity_group': 'ORG', 'score': np.float32(0.99910915), 'word': 'Goldman Sachs', 'start': 0, 'end': 13}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model+tokenizer NER tokens and labels:\n",
            "[CLS]           -> O\n",
            "Goldman         -> B-ORG\n",
            "Sachs           -> I-ORG\n",
            "invested        -> O\n",
            "500             -> O\n",
            "million         -> O\n",
            "dollars         -> O\n",
            "in              -> O\n",
            "a               -> O\n",
            "new             -> O\n",
            "fin             -> O\n",
            "##tech          -> O\n",
            "start           -> O\n",
            "##up            -> O\n",
            ".               -> O\n",
            "[SEP]           -> O\n",
            "\n",
            "================ 6) ZERO-SHOT CLASSIFICATION ================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline zero-shot: {'sequence': 'Bitcoin fell 5% as investors moved money into government bonds.', 'labels': ['crypto', 'macroeconomy', 'stocks'], 'scores': [0.9434500336647034, 0.04950190335512161, 0.007048130501061678]}\n",
            "Model+tokenizer zero-shot scores:\n",
            "stocks      : 0.000\n",
            "crypto      : 0.325\n",
            "macroeconomy: 0.047\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers torch\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForTokenClassification,\n",
        "    # Added explicit imports for the last section\n",
        "    BartTokenizer,\n",
        "    BartForSequenceClassification,\n",
        ")\n",
        "\n",
        "print(\"Torch device available:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "# 1) SENTIMENT ANALYSIS (Finance sentence)\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n================ 1) SENTIMENT ANALYSIS ================\")\n",
        "\n",
        "text = \"The company's stock jumped 7% after the positive earnings report.\"\n",
        "\n",
        "# ---- A) Pipeline version (easy) ----\n",
        "\n",
        "sentiment_pipe = pipeline(\"sentiment-analysis\")\n",
        "print(\"Pipeline output:\", sentiment_pipe(text))\n",
        "\n",
        "# ---- B) Model + tokenizer version ----\n",
        "\n",
        "sentiment_model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "sent_tok = AutoTokenizer.from_pretrained(sentiment_model_id)\n",
        "sent_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_id)\n",
        "\n",
        "inputs = sent_tok(text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = sent_model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "pred_id = torch.argmax(logits, dim=-1).item()\n",
        "label = sent_model.config.id2label[pred_id]\n",
        "\n",
        "print(\"Model+tokenizer label:\", label)\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "# 2) QUESTION ANSWERING (Finance context)\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n================ 2) QUESTION ANSWERING ================\")\n",
        "\n",
        "context = (\n",
        "\"In Q4, Apple reported revenue of 100 billion dollars and a net profit of 25 billion dollars. \"\n",
        "\"The strong iPhone sales contributed to this performance.\"\n",
        ")\n",
        "question = \"What was Apple's net profit?\"\n",
        "\n",
        "# ---- A) Pipeline version ----\n",
        "\n",
        "qa_pipe = pipeline(\"question-answering\")\n",
        "print(\"Pipeline output:\", qa_pipe({\"question\": question, \"context\": context}))\n",
        "\n",
        "# ---- B) Model + tokenizer version ----\n",
        "\n",
        "qa_model_id = \"distilbert-base-uncased-distilled-squad\"\n",
        "\n",
        "qa_tok = AutoTokenizer.from_pretrained(qa_model_id)\n",
        "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_id)\n",
        "\n",
        "qa_inputs = qa_tok(question, context, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    qa_outputs = qa_model(**qa_inputs)\n",
        "\n",
        "start_idx = torch.argmax(qa_outputs.start_logits)\n",
        "end_idx = torch.argmax(qa_outputs.end_logits)\n",
        "\n",
        "answer_ids = qa_inputs[\"input_ids\"][0][start_idx : end_idx + 1]\n",
        "answer_text = qa_tok.decode(answer_ids)\n",
        "\n",
        "print(\"Model+tokenizer answer:\", answer_text)\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "# 3) SUMMARIZATION (Finance news paragraph)\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n================ 3) SUMMARIZATION ================\")\n",
        "\n",
        "finance_paragraph = (\n",
        "\"Microsoft released its quarterly results today. The company reported strong growth \"\n",
        "\"in its cloud business Azure, while personal computing revenue was flat. \"\n",
        "\"Overall, total revenue increased compared to the same quarter last year.\"\n",
        ")\n",
        "\n",
        "# ---- A) Pipeline version ----\n",
        "\n",
        "summ_pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "print(\"Pipeline summary:\", summ_pipe(finance_paragraph, max_length=40, min_length=10, do_sample=False))\n",
        "\n",
        "# ---- B) Model + tokenizer version ----\n",
        "\n",
        "summ_model_id = \"facebook/bart-large-cnn\"\n",
        "\n",
        "summ_tok = AutoTokenizer.from_pretrained(summ_model_id)\n",
        "summ_model = AutoModelForSeq2SeqLM.from_pretrained(summ_model_id)\n",
        "\n",
        "summ_inputs = summ_tok(finance_paragraph, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    summary_ids = summ_model.generate(\n",
        "        **summ_inputs,\n",
        "        max_new_tokens=40,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "\n",
        "summary_text = summ_tok.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(\"Model+tokenizer summary:\", summary_text)\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "# 4) TRANSLATION (Finance phrase EN -> DE)\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n================ 4) TRANSLATION ================\")\n",
        "\n",
        "english_text = \"The central bank decided to increase interest rates by 0.5%.\"\n",
        "\n",
        "# ---- A) Pipeline version ----\n",
        "\n",
        "trans_pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-de\")\n",
        "print(\"Pipeline translation:\", trans_pipe(english_text))\n",
        "\n",
        "# ---- B) Model + tokenizer version ----\n",
        "\n",
        "trans_model_id = \"Helsinki-NLP/opus-mt-en-de\"\n",
        "\n",
        "trans_tok = AutoTokenizer.from_pretrained(trans_model_id)\n",
        "trans_model = AutoModelForSeq2SeqLM.from_pretrained(trans_model_id)\n",
        "\n",
        "trans_inputs = trans_tok(english_text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    trans_ids = trans_model.generate(**trans_inputs, max_new_tokens=40)\n",
        "\n",
        "german_text = trans_tok.decode(trans_ids[0], skip_special_tokens=True)\n",
        "print(\"Model+tokenizer translation:\", german_text)\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "# 5) NER (Named Entity Recognition) on finance sentence\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n================ 5) NER (ENTITY RECOGNITION) ================\")\n",
        "\n",
        "ner_text = \"Goldman Sachs invested 500 million dollars in a new fintech startup.\"\n",
        "\n",
        "# ---- A) Pipeline version ----\n",
        "\n",
        "ner_pipe = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
        "print(\"Pipeline NER:\", ner_pipe(ner_text))\n",
        "\n",
        "# ---- B) Model + tokenizer version ----\n",
        "\n",
        "ner_model_id = \"dslim/bert-base-NER\"\n",
        "\n",
        "ner_tok = AutoTokenizer.from_pretrained(ner_model_id)\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained(ner_model_id)\n",
        "\n",
        "ner_inputs = ner_tok(ner_text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    ner_outputs = ner_model(**ner_inputs)\n",
        "\n",
        "# Get predicted label id for each token\n",
        "\n",
        "pred_ids = torch.argmax(ner_outputs.logits, dim=-1)[0]\n",
        "\n",
        "print(\"Model+tokenizer NER tokens and labels:\")\n",
        "for token_id, label_id in zip(ner_inputs[\"input_ids\"][0], pred_ids):\n",
        "    token_str = ner_tok.decode([token_id]).strip()\n",
        "    label_str = ner_model.config.id2label[label_id.item()]\n",
        "    print(f\"{token_str:15s} -> {label_str}\")\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "# 6) ZERO-SHOT CLASSIFICATION (finance topic)\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n================ 6) ZERO-SHOT CLASSIFICATION ================\")\n",
        "\n",
        "zs_text = \"Bitcoin fell 5% as investors moved money into government bonds.\"\n",
        "candidate_labels = [\"stocks\", \"crypto\", \"macroeconomy\"]\n",
        "\n",
        "# ---- A) Pipeline version ----\n",
        "\n",
        "# This model is specifically for zero-shot classification via NLI (Natural Language Inference)\n",
        "zs_pipe = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "print(\"Pipeline zero-shot:\", zs_pipe(zs_text, candidate_labels=candidate_labels))\n",
        "\n",
        "# ---- B) Model + tokenizer version ----\n",
        "\n",
        "# Removed the redundant re-import\n",
        "zs_model_id = \"facebook/bart-large-mnli\"\n",
        "\n",
        "zs_tok = AutoTokenizer.from_pretrained(zs_model_id)\n",
        "zs_model = AutoModelForSequenceClassification.from_pretrained(zs_model_id)\n",
        "\n",
        "def zero_shot_simple(text, labels):\n",
        "    scores = []\n",
        "    # Ensure the correct label map is used (ENTAILMENT is common for NLI-based zero-shot)\n",
        "    entailment_id = zs_model.config.label2id[\"entailment\"] # Changed 'ENTAILMENT' to 'entailment'\n",
        "    for label in labels:\n",
        "        # premise = text, hypothesis = \"This text is about {label}.\"\n",
        "        pair = zs_tok(text, f\"This text is about {label}.\", return_tensors=\"pt\", truncation=True)\n",
        "        with torch.no_grad():\n",
        "            logits = zs_model(**pair).logits\n",
        "            # For MNLI: labels are ['CONTRADICTION', 'NEUTRAL', 'ENTAILMENT']\n",
        "            probs = torch.softmax(logits, dim=-1)[0]\n",
        "            entailment_score = probs[entailment_id].item()\n",
        "            scores.append(entailment_score)\n",
        "    return scores\n",
        "\n",
        "zs_scores = zero_shot_simple(zs_text, candidate_labels)\n",
        "print(\"Model+tokenizer zero-shot scores:\")\n",
        "for label, score in zip(candidate_labels, zs_scores):\n",
        "    print(f\"{label:12s}: {score:.3f}\")"
      ]
    }
  ]
}